{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1I4BlfWyskRJTs9yJI5LZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliocn/bert_binary_classification_imdb_reviews/blob/main/bert_binary_imdb_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB movie reviews binary classification"
      ],
      "metadata": {
        "id": "KPlYV8aP3dOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "gpj-lHzn3Sip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "fSjGhnH5z1Sh",
        "outputId": "199f1da0-0cb2-45d2-bceb-4c6b7c1783e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q-YbjCkzw0yU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d2eeca-83ef-42b8-95ee-8ec3bd801e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -qU tensorflow-text==2.14.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU tf-models-official==2.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnhqQz6bvDhk",
        "outputId": "39d9d86f-3fc2-4cfd-dc3b-5c4738809758"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import os.path as path\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "hBzZCCAU3qwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pOdqCMoQDRJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f170547c-df1c-4c17-971d-1befa15e7795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 19s 0us/step\n"
          ]
        }
      ],
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "if not path.exists(\"/content/aclImdb\"):\n",
        "  dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "  dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "  train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "  # remove unused folders to make it easier to load the data\n",
        "  remove_dir = os.path.join(train_dir, 'unsup')\n",
        "  shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IwI_2bcIeX8",
        "outputId": "86909548-eccf-40f2-ee47-6a8d63cf090a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 22500 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 2500 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 1\n",
        "validation_split = 0.1\n",
        "\n",
        "# enables operations determinism\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=validation_split,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "\n",
        "train_ds = raw_train_ds.cache()\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=validation_split,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache()\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuxDkcvVIoev",
        "outputId": "9bd24a27-7db8-4a5d-951f-1d4d11152c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b\"I entered the theatre intending to pass a pleasant 90 minutes being entertained if not enlightened. I left neither entertained nor enlightened. This movie can't make up its mind what it wants to be and ends up being not much of anything. There are a few funny lines and a few incredibly pretentious movie references (The 400 Blows--for this character? come off it!). While none of the characters gets treated with much respect, the over thirty gay men get the worst of it: all predatory, fat, sad, slobs. If you're in the mood for a movie dealing with gay relationships check out Parting Glances, Longtime Companion, Trick, All Over the Guy, Red Dirt, Maurice, Philadelphia instead. You'll thank me.<br /><br />\"\n",
            "Label : 0 (neg)\n",
            "Review: b\"Though I liked On the Town better I really liked it. I'm a new comer when it comes to Frank Sinatra and Gene Kelly. Though I had heard of them I had never seen anything with them in it until recently. The first one I saw was Singin in the Rain that made me a fan of Gene's. I think that is better too. But I thought that this movie was good and like all movies there are some parts that are better than others but in my book it's an awesome movie and I love it. Frank and Gene make a good team. I have yet to see them together in Take me out to the Ballgame. But I'm sticking to my guns bu saying that I really enjoyed it, and that I love it!\"\n",
            "Label : 1 (pos)\n",
            "Review: b'Well, I just discovered that there is a show more disgusting and shocking than \"Little Britain\" and I like it! \"The League of Gentlemen\" is a sick British comedy that is about the most awful, insane and disgusting small town in all the UK. This place makes Dibley and Craggy Island (from \"The Vicar of Dibley\" and \"Father Ted\") seem pretty normal!! The format of the show is a lot like LITTLE Britain except that all of it centers around the townspeople of this one hellish town. Both shows feature the same skits again and again every episode and some obviously inspired \"Little Britain\" (particularly the job seeking class skit). But the show differs because although it is crude like \"Little Britain\" (hence not a show for kids), the show has a sick and sadistic quality that sets it apart from all these shows. In particular, animal cruelty and serial killing are recurring themes throughout the show.<br /><br />Now if you haven\\'t guessed, this is NOT a show for kids, the easily offended or normal people and that\\'s probably why I liked it. However, you really do need very thick skin and a love of the awful to enjoy this to the max. Funny and incredibly irreverent beyond belief--you have to see it to believe it.'\n",
            "Label : 1 (pos)\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(f'Review: {text_batch.numpy()[i]}')\n",
        "    label = label_batch.numpy()[i]\n",
        "    print(f'Label : {label} ({class_names[label]})')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabolary"
      ],
      "metadata": {
        "id": "zFLbyiWf317u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGE24yDxh36G",
        "outputId": "a49472b7-d699-4754-a75a-0be89a861ee3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'[UNK]', b'[MASK]', b'[RANDOM]', b'[CLS]', b'[SEP]', b'.', b',']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "words = []\n",
        "with open('/content/aclImdb/imdb.vocab', mode='rt') as vocab:\n",
        "  words = [word.strip('\\n') for word in vocab]\n",
        "\n",
        "_VOCAB = [\n",
        "    # Special tokens\n",
        "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
        "    # Punctuation\n",
        "    b\".\", b\",\", b\";\", b\":\",\n",
        "    # words\n",
        "] + words\n",
        "\n",
        "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
        "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
        "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
        "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
        "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
        "_MAX_SEQ_LEN = 256\n",
        "_MAX_PREDICTIONS_PER_BATCH = 8\n",
        "\n",
        "_VOCAB_SIZE = len(_VOCAB)\n",
        "\n",
        "_VOCAB[:7]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRE-trained Model"
      ],
      "metadata": {
        "id": "7gWm_TX24B67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/giuliocn/bert_binary_classification_imdb_reviews/releases/download/binary-classification/imdb_l10_h256_a4_bert-20231126T160856Z-001.zip'\n",
        "if not path.exists(\"imdb_l10_h256_a4_bert\"):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      'imdb_l10_h256_a4_bert-20231126T160856Z-001.zip',\n",
        "      url,\n",
        "      extract=True, cache_dir='.',\n",
        "      cache_subdir='')"
      ],
      "metadata": {
        "id": "klzQp0h2fdXG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gUEWVskZjEF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70396a00-a277-4980-f88b-976b8c24718e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(32,)]                      0         []                            \n",
            "                                                                                                  \n",
            " pre_layer (Custom>PreLayer  {'input_type_ids': (None,    0         ['input_1[0][0]']             \n",
            " )                           256),                                                                \n",
            "                              'input_word_ids': (None,                                            \n",
            "                             256),                                                                \n",
            "                              'input_mask': (None, 256)                                           \n",
            "                             }                                                                    \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)   {'sequence_output': (None,   1590912   ['pre_layer[0][0]',           \n",
            "                              256, 256),                  1          'pre_layer[0][1]',           \n",
            "                              'default': (None, 256),                'pre_layer[0][2]']           \n",
            "                              'pooled_output': (None, 2                                           \n",
            "                             56),                                                                 \n",
            "                              'encoder_outputs': [(None                                           \n",
            "                             , 256, 256),                                                         \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256),                                                   \n",
            "                              (None, 256, 256)]}                                                  \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 256)                  0         ['BERT_encoder[0][11]']       \n",
            "                                                                                                  \n",
            " classifier (Dense)          (None, 1)                    257       ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15909378 (60.69 MB)\n",
            "Trainable params: 15909377 (60.69 MB)\n",
            "Non-trainable params: 1 (1.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Load model from directory\n",
        "classifier_model = tf.keras.models.load_model('imdb_l10_h256_a4_bert')\n",
        "\n",
        "# Check model architecture\n",
        "classifier_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "DC1oQyl-4P39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on metrics\n",
        "classifier_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.Precision(),\n",
        "        tf.keras.metrics.Recall(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        ])"
      ],
      "metadata": {
        "id": "CXtdtQZll1e8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier_model.evaluate(test_ds, return_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJWUKavFnRlj",
        "outputId": "579ba143-9f73-44d5-a020-e0d5d1d3baa3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 324s 401ms/step - loss: 0.4765 - precision: 0.8324 - recall: 0.8501 - true_positives: 10626.0000 - false_negatives: 1874.0000 - false_positives: 2140.0000 - true_negatives: 10360.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string = f\"\"\"\n",
        "{'Precision':15}  {result['precision']:.3f}\n",
        "{'Recall':15}  {result['recall']:.3f}\n",
        "\n",
        "{'True Positives':15}  {int(result['true_positives'])*100/25e3:2.3f} %\n",
        "{'False Negatives':15}  {int(result['false_negatives'])*100/25e3:2.3f} %\n",
        "{'False Positives':15}  {int(result['false_positives'])*100/25e3:2.3f} %\n",
        "{'True Negatives':15}  {int(result['true_negatives'])*100/25e3:2.3f} %\n",
        "\"\"\"\n",
        "print(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SVXDqymo3Xl",
        "outputId": "e0796dc6-b1bf-46b3-d95c-74f770e412e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision        0.832\n",
            "Recall           0.850\n",
            "\n",
            "True Positives   42.504 %\n",
            "False Negatives  7.496 %\n",
            "False Positives  8.560 %\n",
            "True Negatives   41.440 %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unseen Examples"
      ],
      "metadata": {
        "id": "EsBCsLnpvPIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "\n",
        "for tf_text, tf_label in test_ds.unbatch().shuffle(1).take(5):\n",
        "  examples.append(tf_text.numpy())\n",
        "  print(tf_text.numpy()[:50], f\"Expected output: {tf_label.numpy()}\")\n",
        "  result = classifier_model(tf.constant([tf_text.numpy()] * batch_size))\n",
        "  result = tf.math.reduce_mean(result)\n",
        "  print(f\"model result: {result.numpy():.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsPysGJ7vSk7",
        "outputId": "c2d5eb0c-828a-4e88-d70f-5a097e6ffbff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'This is a tough film to review, since several fact' Expected output: 1\n",
            "model result: 0.008\n",
            "b'This is a charming little film, which like many of' Expected output: 1\n",
            "model result: 0.979\n",
            "b'I remember viewing this movie when I was a kid. I ' Expected output: 0\n",
            "model result: 0.960\n",
            "b'This odd little film starts out with the story of ' Expected output: 0\n",
            "model result: 0.990\n",
            "b\"I have seen a few of Fred Carpenter's movies on Sh\" Expected output: 1\n",
            "model result: 0.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQGR2Db_06Gb",
        "outputId": "d90e1f3b-ca63-4114-e2f5-45e26361179a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"I remember viewing this movie when I was a kid. I recall it terrified me immensely and it stayed with me all these years. I spent a couple of years trying to find it online...didn't remember the title, only the storyline. After searching and searching, I came across a VHS that was being sold on E-Bay. I was excited and when it finally arrived, I jammed it into the VCR and couldn't help but feel a bit nostalgic. Needless to say, I was slightly disappointed. This wasn't the movie I remember watching as a kid. It was boring at times and I found Beryl Reid's incessant whinning extremely annoying. Both performances by Reid and Flora Robson were good overall but the movie wasn't scary. I think any movie is worth viewing to form you're own opinion but sometimes, well......\""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Examples"
      ],
      "metadata": {
        "id": "xglADFs2f0LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier_model(tf.constant(['The movie was great!'] * batch_size))\n",
        "result = tf.math.reduce_mean(result)\n",
        "print(f\"model result: {result.numpy():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEmm8-dW2iIr",
        "outputId": "fd023b2a-9cd8-4f56-e6f2-28a46bc0fca6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model result: 0.946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier_model(tf.constant(['The movie was terrible!'] * batch_size))\n",
        "result = tf.math.reduce_mean(result)\n",
        "print(f\"model result: {result.numpy():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33e56a8-0564-4cad-8369-b0e8c6a9eb49",
        "id": "vRJ7mNqYf_s0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model result: 0.211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model fails to classify single positive words\n",
        "result = classifier_model(tf.constant(['Wonderful.'] * batch_size))\n",
        "result = tf.math.reduce_mean(result)\n",
        "print(f\"model result: {result.numpy():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bad77e5-97a9-4ed0-faa4-13badbbc9cb1",
        "id": "wdHMHpmMgFsG"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model result: 0.180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier_model(tf.constant(['Terrible.'] * batch_size))\n",
        "result = tf.math.reduce_mean(result)\n",
        "print(f\"model result: {result.numpy():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7265fd3-3cb3-4ae8-b935-9c3f8de85895",
        "id": "W9p5ed5CgQ6D"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model result: 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neutral statement\n",
        "result = classifier_model(tf.constant(['This is a movie'] * batch_size))\n",
        "result = tf.math.reduce_mean(result)\n",
        "print(f\"model result: {result.numpy():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcb7ad9-1b02-4f96-b1e9-a1600b45556b",
        "id": "IP8RUiTygXXi"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model result: 0.520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classify single words with a template string"
      ],
      "metadata": {
        "id": "LmIAU74ioUJO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VBWzH6exlCPS"
      },
      "outputs": [],
      "source": [
        "good_words = [\n",
        "    # Good\n",
        "    'good','great','enjoyable','amazing','delightful','lovely','pleasant',]\n",
        "bad_words = [\n",
        "    # Bad\n",
        "    'bad','poor','inferior','lacking','awful','terrible','abominable',]\n",
        "neutral_words = [\n",
        "    # Neutral\n",
        "    'indifferent','mediocre','ordinary','average','commonplace','medium','moderate',\n",
        "]\n",
        "\n",
        "def print_my_examples(words):\n",
        "  for w in words:\n",
        "    result = classifier_model(tf.constant([f\"This movie was {w}\"] * 32))\n",
        "    result = tf.math.reduce_mean(result)\n",
        "    print(f\"input: {w:15}\\t\\t result:{result:.3f}\", sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_my_examples(good_words)\n",
        "print('\\n')\n",
        "print_my_examples(bad_words)\n",
        "print('\\n')\n",
        "print_my_examples(neutral_words)"
      ],
      "metadata": {
        "id": "RnxyfX_9AVIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc92152-5641-4a0e-85d6-5dbe892d8e17"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: good           \t\t result:0.668\n",
            "input: great          \t\t result:0.790\n",
            "input: enjoyable      \t\t result:0.770\n",
            "input: amazing        \t\t result:0.834\n",
            "input: delightful     \t\t result:0.716\n",
            "input: lovely         \t\t result:0.693\n",
            "input: pleasant       \t\t result:0.749\n",
            "\n",
            "\n",
            "input: bad            \t\t result:0.085\n",
            "input: poor           \t\t result:0.075\n",
            "input: inferior       \t\t result:0.096\n",
            "input: lacking        \t\t result:0.071\n",
            "input: awful          \t\t result:0.059\n",
            "input: terrible       \t\t result:0.083\n",
            "input: abominable     \t\t result:0.021\n",
            "\n",
            "\n",
            "input: indifferent    \t\t result:0.129\n",
            "input: mediocre       \t\t result:0.131\n",
            "input: ordinary       \t\t result:0.496\n",
            "input: average        \t\t result:0.264\n",
            "input: commonplace    \t\t result:0.747\n",
            "input: medium         \t\t result:0.060\n",
            "input: moderate       \t\t result:0.426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eezWprTVEBK"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}